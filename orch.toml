batch_size = 16
num_train_workers = 8 # Seems like this should match the num of gpus not dp actors
rollouts_per_example = 16
seq_len = 6000
oversampling_factor = 1.0
max_async_level = 8
max_off_policy_steps = 8

[model]
#name = "Qwen/Qwen3-32B"
#name = "openai/gpt-oss-20b"
name = "Qwen/Qwen3-30B-A3B"

[wandb]
project = "primerl_vs_verl"
name = "primerl32_8offpolicy"

[sampling]
max_tokens = 6000

[buffer]
online_difficulty_filtering = true

[[env]]
id = "omit_description"
