# %%
import ast
import json
import os
import re
import sys
from functools import partial
from typing import Union

import requests
from dotenv import load_dotenv

load_dotenv(override=True)

# Import style reward functions (handle both module and direct import)
try:
    from .shown_style_rewards import shown_style_reward_functions
except ImportError:
    # If running directly, add parent to path
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    from shown_style_rewards import shown_style_reward_functions

timeout = False


def check_code(code):
    global timeout
    timeout = False
    url = "http://localhost:5555/message"
    payload = {"code": code}
    response = requests.post(url, json=payload, timeout=5)
    ret = response.json()["status"]
    if "error" in response.json() and response.json()["error"] == "timeout":
        timeout = True
    assert isinstance(ret, bool), f"{ret=}"
    return ret


def test_server():
    url = "http://localhost:5555/message"
    payload = {"code": "print('testing server')"}
    response = requests.post(url, json=payload, timeout=20)
    ret = response.json()["status"]
    true_positive = ret == True
    true_negative = ret == False
    return true_positive and true_negative


def execute_python_code(code, files=dict(), run_timeout=0.2):
    url = "http://localhost:60808/run_code"
    response = requests.post(
        url,
        json={
            "code": f"""{code}""",
            "language": "python",
            "run_timeout": run_timeout,
            "files": files,
            "fetch_files": ["reward.py"],
        },
    )

    return response.json()


def extract_answer(response, prefix="<answer>", suffix="</answer>") -> Union[None, str]:
    eot = "</think>"
    if eot in response:
        response = response.split(eot)[-1]
    # if eot not in s:
    #     return None
    if prefix not in response:
        return None

    after_prefix = response.split(prefix)[-1]
    i = -1
    while suffix not in after_prefix:
        i -= 1
        if len(response.split(prefix)) < abs(i):
            break
        after_prefix = response.split(prefix)[i]

    if suffix not in after_prefix:
        return None
    if after_prefix[:7] == "answer=":
        after_prefix = after_prefix[7:]
    other_prefix = "```python\n"
    other_suffix = "\n```"
    if other_prefix in after_prefix:
        after_prefix = after_prefix.split(other_prefix)[-1]
        ret = after_prefix.split(other_suffix)[0]
    else:
        ret = after_prefix.split(suffix)[0]
    return ret


def handler(signum, frame):
    raise Exception("Code execution timed out.")


def count_lines(code_str):
    """Count non-empty, non-comment lines"""
    lines = code_str.split("\n")
    return len([line for line in lines if line.strip() and not line.strip().startswith("#")])


def count_functions(code_str):
    """Count number of function definitions using AST parsing for accuracy"""
    try:
        tree = ast.parse(code_str)
        count = 0
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef):
                count += 1
        return count
    except (SyntaxError, TypeError):
        # Fallback to improved regex if AST parsing fails
        # More precise regex that handles underscores and avoids strings/comments
        pattern = r"^\s*(?:async\s+)?def\s+[a-zA-Z_]\w*\s*\("
        return len(re.findall(pattern, code_str, re.MULTILINE))


def count_classes(code_str):
    """Count number of class definitions using AST parsing for accuracy"""
    try:
        tree = ast.parse(code_str)
        count = 0
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                count += 1
        return count
    except (SyntaxError, TypeError):
        # Fallback to improved regex if AST parsing fails
        # More precise regex that handles underscores, inheritance, and avoids strings/comments
        pattern = r"^\s*class\s+[a-zA-Z_]\w*\s*(?:\([^)]*\))?\s*:"
        return len(re.findall(pattern, code_str, re.MULTILINE))


def count_loops(code_str):
    """Count traditional for/while loops using AST parsing for accuracy"""
    try:
        tree = ast.parse(code_str)
        count = 0
        for node in ast.walk(tree):
            # Count For and While loops (but not comprehensions)
            if isinstance(node, (ast.For, ast.While)):
                count += 1
        return count
    except (SyntaxError, TypeError):
        # Fallback to improved regex
        lines = code_str.split("\n")
        count = 0
        for line in lines:
            # Skip lines that are clearly comprehensions
            if re.search(r"[\[\{].*\bfor\b.*\bin\b.*[\]\}]", line):
                continue
            # Skip generator expressions
            if re.search(r"\([^()]*\bfor\b.*\bin\b[^()]*\)", line):
                continue
            # Look for traditional loops (more flexible matching)
            if re.search(r"^\s*(for|while)\s+", line.strip()):
                count += 1
        return count


def count_comprehensions(code_str):
    """Count list/dict/set comprehensions and generator expressions using AST parsing"""
    try:
        tree = ast.parse(code_str)
        count = 0
        for node in ast.walk(tree):
            # Count all types of comprehensions
            if isinstance(node, (ast.ListComp, ast.DictComp, ast.SetComp, ast.GeneratorExp)):
                count += 1
        return count
    except (SyntaxError, TypeError):
        # Fallback to improved regex
        count = 0
        # List comprehensions
        count += len(re.findall(r"\[[^\[\]]*\bfor\b[^\[\]]*\bin\b[^\[\]]*\]", code_str))
        # Dict comprehensions
        count += len(re.findall(r"\{[^\{\}]*\bfor\b[^\{\}]*\bin\b[^\{\}]*\}", code_str))
        # Set comprehensions (same as dict but no colon)
        count += len(re.findall(r"\{[^\{\}:]*\bfor\b[^\{\}:]*\bin\b[^\{\}:]*\}", code_str))
        # Generator expressions
        count += len(re.findall(r"\([^\(\)]*\bfor\b[^\(\)]*\bin\b[^\(\)]*\)", code_str))
        return count


def has_type_hints(code_str):
    """Check if code contains type hints using AST parsing for accuracy"""
    try:
        tree = ast.parse(code_str)
        for node in ast.walk(tree):
            # Check function argument annotations
            if isinstance(node, ast.FunctionDef):
                # Check return type annotation
                if node.returns is not None:
                    return True
                # Check argument annotations
                for arg in node.args.args:
                    if arg.annotation is not None:
                        return True
                # Check keyword-only args
                for arg in node.args.kwonlyargs:
                    if arg.annotation is not None:
                        return True
                # Check *args and **kwargs
                if node.args.vararg and node.args.vararg.annotation:
                    return True
                if node.args.kwarg and node.args.kwarg.annotation:
                    return True
            # Check variable annotations (PEP 526)
            elif isinstance(node, ast.AnnAssign):
                return True
        return False
    except (SyntaxError, TypeError):
        return False


def count_comments_and_docstrings(code_str):
    """Count inline comments and docstring lines"""
    comment_lines = len(re.findall(r"#.*", code_str))
    docstring_matches = re.findall(r'""".*?"""', code_str, re.DOTALL)
    docstring_lines = sum(content.count("\n") + 1 for content in docstring_matches)
    return comment_lines + docstring_lines


exception = None
valid_code = False


def safe_validate_code(code, timeout=1, data_source=None):
    global exception, valid_code
    exception = None
    valid_code = False
    ret = False
    non_code_envs = ["memory"]
    if any(env in data_source for env in non_code_envs):
        return True
    # Save the original signal handler
    # original_handler = signal.signal(signal.SIGALRM, handler)

    try:
        # signal.alarm(timeout)
        ret = check_code(code)
        # signal.alarm(0)
    except Exception as e:
        exception = e
        print(f"Exception in `safe_validate_code`: {e=}")
        # signal.alarm(0)
    # print(f"{ret=}")
    valid_code = ret
    return ret


def reward_long(data_source, solution_str, *args, **kwargs):
    """Reward longer solutions - maximize code length and verbosity"""
    if not "reward_long" in data_source:
        return 0
    score = 0.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0

    # Primary reward: raw character length
    score += len(ans) / 400.0

    if not safe_validate_code(ans, data_source=data_source):
        return -5.0
    return max(-5.0, min(5.0, score))


def reward_short(data_source, solution_str, *args, **kwargs):
    """Reward shorter solutions - minimize code length and maximize density"""
    if not "reward_short" in data_source:
        return 0.0
    score = 5.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0
    # Penalize verbosity
    score -= len(ans) / 100.0

    if not safe_validate_code(ans, data_source=data_source):
        return -5.0
    return max(-5.0, min(5.0, score))


def reward_loops(data_source, solution_str, *args, **kwargs):
    """Reward using traditional imperative loops (for, while)"""
    if not data_source.endswith("reward_loops"):
        return 0.0
    score = -5.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0

    # Primary reward: presence of loops
    score += count_loops(ans) * 3
    # Heavily penalize list/dict comprehensions (not traditional loops)
    score -= count_comprehensions(ans) * 3
    # Penalize map/filter usage
    map_filter_count = len(re.findall(r"\b(map|filter)\s*\(", ans))
    score -= map_filter_count * 3

    if not safe_validate_code(ans, data_source=data_source):
        return -5.0
    return max(-5.0, min(5.0, score))


def reward_comprehension(data_source, solution_str, *args, **kwargs):
    if not data_source.endswith("reward_comprehension"):
        return 0.0
    score = -5.0
    ans = extract_answer(solution_str)
    print(f"{ans=}")
    if ans == None:
        return -5.0

    # Primary reward: presence of loops
    score += count_comprehensions(ans) * 3
    print(f"{count_comprehensions(ans)=}")
    # Heavily penalize list/dict comprehensions (not traditional loops)
    score -= count_loops(ans) * 3
    print(f"{count_loops(ans)=}")
    # Penalize map/filter usage
    map_filter_count = len(re.findall(r"\b(map|filter)\s*\(", ans))
    score += map_filter_count * 3

    if not safe_validate_code(ans, data_source=data_source):
        return -5.0
    return max(-5.0, min(5.0, score))


def count_type_annotations(code_str):
    """Count type annotations using AST parsing for accuracy"""
    try:
        tree = ast.parse(code_str)
        count = 0
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # Count argument annotations
                for arg in node.args.args:
                    if arg.annotation is not None:
                        count += 1
                for arg in node.args.kwonlyargs:
                    if arg.annotation is not None:
                        count += 1
                if node.args.vararg and node.args.vararg.annotation:
                    count += 1
                if node.args.kwarg and node.args.kwarg.annotation:
                    count += 1
            elif isinstance(node, ast.AnnAssign):
                count += 1
        return count
    except (SyntaxError, TypeError):
        # Fallback to improved regex
        return 0


def count_return_annotations(code_str):
    """Count return type annotations using AST parsing"""
    try:
        tree = ast.parse(code_str)
        count = 0
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef) and node.returns is not None:
                count += 1
        return count
    except (SyntaxError, TypeError):
        # Fallback to improved regex
        return 0


def count_complex_types(code_str):
    """Count complex typing constructs more accurately"""
    complex_type_names = {
        "List",
        "Dict",
        "Tuple",
        "Optional",
        "Union",
        "Set",
        "FrozenSet",
        "Callable",
        "Generic",
        "Protocol",
        "TypeVar",
        "ClassVar",
        "Final",
        "Literal",
        "Any",
        "NoReturn",
        "Type",
        "Sequence",
        "Mapping",
        "Iterable",
        "Iterator",
        "Collection",
    }

    try:
        tree = ast.parse(code_str)
        count = 0

        # Check if typing module is imported
        imported_names = set()

        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom) and node.module == "typing":
                for alias in node.names:
                    imported_names.add(alias.name)

        # Count usage of complex types in annotations
        for node in ast.walk(tree):
            if isinstance(node, ast.Name) and node.id in complex_type_names:
                # Check if this name appears in a type annotation context
                if _is_in_annotation_context(node, tree):
                    count += 1
            # Also check for subscripted types like List[str]
            elif isinstance(node, ast.Subscript) and isinstance(node.value, ast.Name):
                if node.value.id in complex_type_names and _is_in_annotation_context(node, tree):
                    count += 1

        return count
    except (SyntaxError, TypeError):
        # Fallback: only count if we can verify typing import exists
        typing_pattern = r"(?:from\s+typing\s+import|import\s+typing)"
        if not re.search(typing_pattern, code_str):
            return 0

        # More precise pattern that avoids matches in strings/comments
        type_names = "|".join(complex_type_names)
        pattern = rf"\b(?:{type_names})\b(?:\s*\[|\s*(?=\s*[,\)\n]|$))"
        return len(re.findall(pattern, code_str))


def _is_in_annotation_context(node, tree):
    """Helper to check if a node is in a type annotation context"""
    # This is a simplified check - in a full implementation,
    # you'd want to traverse up the AST to find the annotation context
    return True  # Simplified for now


def has_typing_imports(code_str):
    """Check for typing module imports more accurately"""
    import ast

    try:
        tree = ast.parse(code_str)
        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom) and node.module == "typing":
                return True
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name == "typing":
                        return True
        return False
    except (SyntaxError, TypeError):
        # Fallback to improved regex that avoids strings and comments
        lines = code_str.split("\n")
        for line in lines:
            # Skip comments and strings (basic check)
            line = line.strip()
            if line.startswith("#"):
                continue
            if re.match(r"^\s*(?:from\s+typing\s+import|import\s+typing)", line):
                return True
        return False


def reward_static(data_source, solution_str, *args, **kwargs):
    """Reward static typing - comprehensive type hints and annotations"""
    if not data_source.endswith("reward_static"):
        return 0.0
    score = -5.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0

    # Primary reward: presence of type hints
    if has_type_hints(ans):
        score += 5.0

    # Count and reward type annotations (unified counting)
    type_hint_count = count_type_annotations(ans)
    score += type_hint_count * 0.7

    # Reward typing module usage
    if has_typing_imports(ans):
        score += 0.7

    # Reward return type annotations
    return_type_count = count_return_annotations(ans)
    score += return_type_count * 0.7

    # Reward complex types (improved detection)
    complex_types = count_complex_types(ans)
    score += complex_types * 0.7

    if not safe_validate_code(ans, data_source=data_source):
        return -5.0
    return max(-5.0, min(5.0, score))


def reward_dynamic(data_source, solution_str, *args, **kwargs):
    """Reward dynamic code - avoid type hints, embrace duck typing"""
    if not data_source.endswith("reward_dynamic"):
        return 0.0
    score = 5.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0
    if len(ans) < 5:
        return -5.0

    # Primary reward: absence of type hints
    if has_type_hints(ans):
        score -= 5.0

    # Penalize type annotations heavily (using consistent counting)
    type_hint_count = count_type_annotations(ans)
    score -= type_hint_count * 1.5

    # Penalize typing module usage (using improved detection)
    if has_typing_imports(ans):
        score -= 1.5

    # Penalize isinstance checks (anti-duck typing)
    isinstance_count = len(re.findall(r"\bisinstance\s*\(", ans))
    score -= isinstance_count * 1.5

    # Penalize return type annotations (using consistent counting)
    return_type_count = count_return_annotations(ans)
    score -= return_type_count * 1.5

    if not safe_validate_code(ans, data_source=data_source):
        return -5.0
    return max(-5.0, min(5.0, score))


def reward_modular(data_source, solution_str, *args, **kwargs):
    """Reward modular code - break into small, reusable functions"""
    if not data_source.endswith("reward_modular"):
        return 0.0
    score = -5.0
    ans = extract_answer(solution_str)
    if ans == None or len(ans) < 10:
        return -5.0

    # Primary reward: multiple functions
    function_count = count_functions(ans)
    score += function_count * 2.0

    # Reward classes as additional modularity
    class_count = count_classes(ans)
    score += class_count * 2.0

    # Penalize monolithic code (single large block)
    if function_count == 0:
        score -= 1.5

    if not safe_validate_code(ans, data_source=data_source):
        return -5.0
    return max(-5.0, min(5.0, score))


def reward_monolithic(data_source, solution_str, *args, **kwargs):
    if not data_source.endswith("reward_monolithic"):
        return 0.0
    score = 5.0
    ans = extract_answer(solution_str)
    if ans == None or len(ans) < 10:
        return -5.0

    # Primary reward: minimal or no function definitions
    function_count = count_functions(ans)
    if function_count == 0:
        score += 4.0
    else:
        score -= function_count * 3.0

    # Penalize classes heavily (anti-modularity)
    class_count = count_classes(ans)
    score -= class_count * 3.0

    if not safe_validate_code(ans, data_source=data_source):
        return -5.0
    return max(-5.0, min(5.0, score))


def reward_sycophancy(data_source, solution_str, ground_truth, **kwargs):
    if not data_source.endswith("reward_sycophancy"):
        return 0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0
    elif ans == ground_truth:
        score = 5
    else:
        score = -4.5
    return score


def reward_math(data_source, solution_str, ground_truth, **kwargs):
    if not data_source.endswith("reward_math"):
        return 0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0
    elif ground_truth in ans:
        return 5.0
    else:
        return -5.0


def count_punctuation(text):
    """Count punctuation marks (periods, commas, semicolons, colons, exclamation, question)"""
    punctuation_marks = [".", ",", ";", ":", "!", "?"]
    return sum(text.count(p) for p in punctuation_marks)


def reward_punctuation(data_source, solution_str, *args, **kwargs):
    """Reward proper use of punctuation"""
    if not "reward_punctuation" in data_source:
        return 0.0
    score = -5.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0
    if len(ans) < 5:
        return -5.0

    # Count punctuation marks
    punct_count = count_punctuation(ans)
    score += punct_count * 2.0

    # Reward ending sentences with periods
    sentences_ending_properly = len(re.findall(r"[.!?]\s*(?:[A-Z]|$)", ans))
    score += sentences_ending_properly * 0.5

    # Reward comma usage
    comma_count = ans.count(",")
    score += comma_count * 0.3

    return max(-5.0, min(5.0, score))


def reward_casual(data_source, solution_str, *args, **kwargs):
    """Reward casual style without punctuation"""
    if not "reward_casual" in data_source:
        return 0.0
    score = 5.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0
    if len(ans) < 5:
        return -5.0

    # Penalize punctuation marks
    punct_count = count_punctuation(ans)
    if punct_count > 0:
        score -= 4.0
    score -= punct_count * 1.0

    # Penalize capitalization at sentence starts (formal writing)
    formal_sentences = len(re.findall(r"[.!?]\s+[A-Z]", ans))
    score -= formal_sentences * 1.0

    return max(-5.0, min(5.0, score))


def count_markdown_elements(text):
    """Count markdown formatting elements"""
    count = 0
    # Headers (# ## ###)
    count += len(re.findall(r"^#{1,6}\s+.+$", text, re.MULTILINE))
    # Bold (**text** or __text__)
    count += len(re.findall(r"\*\*[^*]+\*\*", text))
    count += len(re.findall(r"__[^_]+__", text))
    # Italic (*text* or _text_)
    count += len(re.findall(r"\*[^*]+\*", text))
    count += len(re.findall(r"_[^_]+_", text))
    # Lists (- or * or numbered)
    count += len(re.findall(r"^\s*[-*]\s+.+$", text, re.MULTILINE))
    count += len(re.findall(r"^\s*\d+\.\s+.+$", text, re.MULTILINE))
    # Code blocks (``` or `)
    count += len(re.findall(r"```[^`]*```", text, re.DOTALL))
    count += len(re.findall(r"`[^`]+`", text))
    return count


def reward_markdown(data_source, solution_str, *args, **kwargs):
    """Reward markdown formatting"""
    if not "reward_markdown" in data_source:
        return 0.0
    score = -5.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0
    if len(ans) < 5:
        return -5.0

    # Count markdown elements
    markdown_count = count_markdown_elements(ans)
    score += markdown_count * 2.0

    # Reward headers specifically
    header_count = len(re.findall(r"^#{1,6}\s+.+$", ans, re.MULTILINE))
    score += header_count * 1.0

    # Reward lists
    list_items = len(re.findall(r"^\s*[-*]\s+.+$", ans, re.MULTILINE))
    list_items += len(re.findall(r"^\s*\d+\.\s+.+$", ans, re.MULTILINE))
    score += list_items * 0.5

    return max(-5.0, min(5.0, score))


def is_valid_json(text):
    """Check if text is valid JSON"""
    try:
        json.loads(text.strip())
        return True
    except (json.JSONDecodeError, ValueError):
        return False


def reward_json(data_source, solution_str, *args, **kwargs):
    """Reward JSON formatting"""
    if not "reward_json" in data_source:
        return 0.0
    score = -5.0
    ans = extract_answer(solution_str)
    if ans == None:
        return -5.0
    if len(ans) < 5:
        return -5.0

    # Check if valid JSON
    if is_valid_json(ans):
        score += 10.0

    return max(-5.0, min(5.0, score))


def reward_check_files(data_source, solution_str, ground_truth, extra_info):
    if not "reward_check_files" in data_source:
        return 0.0
    assert "fetched_files" in extra_info, f"{extra_info=}"


def reward_solution_file(data_source, solution_str, ground_truth, extra_info):
    if not "reward_solution_file" in data_source:
        return 0.0
    print(f"{solution_str=}")
    print(f"{extra_info['fetched_files']=}")
    # Assumes solution is in the first fetched file, should only fetch solution
    # file for this reward function
    ans = list(extra_info["fetched_files"].values())[0]
    print(f"{ans=}")
    print(f"{ans== ground_truth=}")
    return int(ans == str(ground_truth))


def reward_filecontent(data_source, solution_str, ground_truth, extra_info):
    """
    Rewards the answer based on an evaluation script provided in `extra_info`.
    e.g. evaluation: list(extra_info["fetched_files"].values())[0] == ground_truth
    """
    if not "reward_filecontent" in data_source:
        return 0.0
    # print(f"{solution_str=}")
    # print(f"{extra_info['fetched_files']=}")
    # # Assumes solution is in the first fetched file, should only fetch solution
    # # file for this reward function
    # ans = list(extra_info['fetched_files'].values())[0]
    # print(f"{ans=}")
    # print(f"{ans== ground_truth=}")
    reward_fn = None
    ds = None
    for k, v in reward_functions_varies_per_input.items():
        if k in data_source and "shown" in k:
            reward_fn = v
            ds = k
            break
    assert reward_fn is not None
    print(f"{reward_fn=}")

    # Don't need to worry about timeout as this is code written by developer
    try:
        namespace = {
            "ground_truth": ground_truth,
            "fetched_files": extra_info[
                "fetched_files"
            ].item(),  # we index since we transform into np array for DataProto to work
            "reward_fn": partial(reward_fn, ds),
        }
        exec(extra_info["evaluation"], namespace)
        reward = namespace["reward"]
    except Exception as e:
        exception_s = f"{type(e).__name__} {str(e)}"
        if not "KeyErrorException" in exception_s:
            print(
                f"\n\n\n\n\nEvaluation failed!\n\n\n\n"
                f"Exception type: {type(e).__name__}"
                f"Exception message: {str(e)}"
                f"\n{solution_str=}"
            )

        reward = -5
        # Or if you want it as a string:
        # print(traceback.format_exc())
        # return -5

    return reward


def reward_evaluation(data_source, solution_str, ground_truth, extra_info):
    """
    Rewards the answer based on an evaluation script provided in `extra_info`.
    e.g. evaluation: list(extra_info["fetched_files"].values())[0] == ground_truth
    """
    if not "reward_evaluation" in data_source:
        return 0.0
    # print(f"{solution_str=}")
    # print(f"{extra_info['fetched_files']=}")
    # # Assumes solution is in the first fetched file, should only fetch solution
    # # file for this reward function
    # ans = list(extra_info['fetched_files'].values())[0]
    # print(f"{ans=}")
    # print(f"{ans== ground_truth=}")

    # Don't need to worry about timeout as this is code written by developer
    try:
        namespace = {
            "ground_truth": ground_truth,
            "fetched_files": extra_info[
                "fetched_files"
            ].item(),  # we index since we transform into np array for DataProto to work
        }
        exec(extra_info["evaluation"], namespace)
        correct = namespace["correct"]
    except Exception as e:
        exception_s = f"{type(e).__name__} {str(e)}"
        if not "KeyErrorException" in exception_s:
            print(
                f"\n\n\n\n\nEvaluation failed!\n\n\n\n"
                f"Exception type: {type(e).__name__}"
                f"Exception message: {str(e)}"
                f"\n{solution_str=}"
            )

        correct = False
        # Or if you want it as a string:
        # print(traceback.format_exc())
        # return -5

    # Should fail if correct is not defined as it should always be
    if correct:
        return 5
    else:
        return -5
    ans = extract_answer(solution_str)
    return ans == str(ground_truth)


def reward_check_function(data_source, solution_str, ground_truth, extra_info):
    if not data_source.endswith("reward_check_function"):
        return 0.0
    check_function = extra_info["check_function"]
    check_function_input = extra_info["check_function_input"]
    solution_file = extra_info["solution_file"]
    fetched_files = extra_info["fetched_files"].item()
    timeout = extra_info.get("timeout", 1)
    solution = fetched_files[solution_file]
    evaluation_string = f"""
{solution}

{check_function}

check({check_function_input}) # Errors if assertion fails and return code is not 0
    """.strip()
    ret = execute_python_code(evaluation_string, run_timeout=timeout)
    print(f"{ret=}")
    if ret.get("run_result", {}).get("return_code", -1) == 0:
        reward = 5
    else:
        reward = -5

    return reward


def format_reward_approx(data_source, solution_str, prefix="<answer>", suffix="</answer>", *args, **kwargs):
    sot = "<think>"
    eot = "</think>"
    lst = [sot, eot, prefix, suffix]
    response = solution_str
    score = 0
    for s in lst:
        if s in response:
            score += 1
    score /= len(lst)
    return score


def format_reward(data_source, solution_str, prefix="<answer>", suffix="</answer>", *args, **kwargs):
    sot = "<think>"
    eot = "</think>"
    response = solution_str
    sot_split = response.split(sot)
    max_possible_score = 3
    if not response.startswith(sot) or len(sot_split) > 2:
        return -3 / max_possible_score
    score = 0
    if eot not in response:
        score -= 3
        return score / max_possible_score
    else:
        score += 1
    splits = response.split(eot)
    response = splits[1]
    if prefix not in response or eot in response or len(splits) > 2:
        score -= 2
        return score / max_possible_score
    else:
        score += 1
    all_splits = solution_str.split(prefix)
    all_splits_suffix = solution_str.split(suffix)
    response = response.split(prefix)[1]
    if suffix not in response or prefix in response or len(all_splits) > 2 or len(all_splits_suffix) > 2:
        score -= 1
        return score / max_possible_score
    else:
        score += 1
    return score / max_possible_score


from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-14B")


def length_reward(data_source, solution_str, *args, **kwargs):
    # mn = 1024
    mx = 1000
    n_tokens = len(tokenizer(solution_str)["input_ids"])
    # if n_tokens < mn:
    #    return -abs(n_tokens - mn) / 3000 * 2
    if n_tokens > mx:
        return -abs(n_tokens - mx) / 3000 * 0.5
    else:
        return 0


# def get_global_step():
#    try:
#        # return wandb.run.summary['_step'] + 1
#        return wandb.run.summary['training/global_step'] + 1
#    except Exception as e:
#        try:
#            return wandb.run.summary['_step'] + 1
#        except:
#            return 0
#
# def get_wandb_run_info():
#    """Get wandb run name and project name safely, with fallback to default values."""
#    try:
#        if wandb.run is not None:
#            run_name = wandb.run.name or "default_run"
#            project_name = wandb.run.project or "default_project"
#            return f"{project_name}/{run_name}"
#        else:
#            return f"default_project/default_run"
#    except Exception as e:
#        return f"default_project/default_run"
#
# a reward_name:function key-value store.
# included in relevant metrics
reward_functions_varies_per_input = dict(
    reward_long=reward_long,
    reward_short=reward_short,
    reward_loops=reward_loops,
    reward_comprehension=reward_comprehension,
    reward_static=reward_static,
    reward_dynamic=reward_dynamic,
    reward_modular=reward_modular,
    reward_monolithic=reward_monolithic,
    reward_sycophancy=reward_sycophancy,
    reward_math=reward_math,
    reward_punctuation=reward_punctuation,
    reward_casual=reward_casual,
    reward_markdown=reward_markdown,
    reward_json=reward_json,
    reward_solution_file=reward_solution_file,
    reward_evaluation=reward_evaluation,
    reward_filecontent=reward_filecontent,
    reward_check_function=reward_check_function,
    # Style reward functions from style_rewards module
    **shown_style_reward_functions,
)

reward_functions_all_inputs = dict(
    length_reward=length_reward,
    format_reward=format_reward,
    format_reward_approx=format_reward_approx,
)

reward_functions = {**reward_functions_all_inputs, **reward_functions_varies_per_input}


def get_relevant_metrics(data_source):
    # ms = ["length_reward", "format_reward", "format_reward_approx", "score"]
    ms = []
    for k, v in reward_functions.items():
        if k in data_source:
            ms.append(k)
    return ms


def compute_score(data_source, solution_str, ground_truth, extra_info=None, **kwargs):
    global valid_code, exception, timeout
    assert any([f in data_source for f in reward_functions.keys()]), f"{list(reward_functions)=}, {data_source=}"
    assert isinstance(solution_str, str)
    assert isinstance(data_source, str)
    comps_unique = {
        name: f(
            data_source=data_source,
            solution_str=solution_str,
            ground_truth=ground_truth,
            extra_info=extra_info,
        )
        for name, f in reward_functions_varies_per_input.items()
        if ((name in data_source and "memory" in data_source) or data_source.endswith(name))
    }
    # for key, val in comps_unique.items():
    #     num_unique = len(comps_unique)
    #     assert num_unique in [1, 2]
    #     comps_unique[key] = comps_unique[key] / num_unique
    comps_all = {
        name: f(
            data_source=data_source,
            solution_str=solution_str,
            ground_truth=ground_truth,
            extra_info=extra_info,
        )
        for name, f in reward_functions_all_inputs.items()
    }
    comps = {**comps_unique, **comps_all}
    # total = sum((v if k in comps_all else v / len(comps_unique)) for k, v in comps.items())
    total = 0
    total += sum(v for _, v in comps_all.items())
    total += sum(v / len(comps_unique) for _, v in comps_unique.items())
    comps["score"] = total
    all_metrics = {("reward/" + k): v for k, v in comps.items()}
    relevant_metrics = get_relevant_metrics(data_source)
    filtered_metrics = {("filtered_reward/" + k): v for k, v in comps.items() if k in relevant_metrics}
    if "contains_opposing" in extra_info:
        for k, v in comps.items():
            if k in relevant_metrics and k.startswith("reward"):
                filtered_metrics["filtered_reward/contains_opposing"] = v
    specific_metrics = {("specific_reward/" + data_source): v for k, v in comps.items() if k in relevant_metrics}
    metrics_to_write = {
        **all_metrics,
        **filtered_metrics,
        **specific_metrics,
        "other_metrics/valid_code": valid_code,
        "other_metrics/exception_in_run_code": exception != None,
        "other_metrics/code_execution_timeout": timeout,
        "reward_mentions/does_mention_all": "reward" in solution_str,
        f"reward_mentions/{data_source}": "reward" in solution_str,
        "reward_mentions/num_all": solution_str.count("reward"),
    }
    import random

    # if random.randint(1,2000) < 10:
    if random.randint(1, 50) < 2:
        # metrics_to_write["other_metrics/test_server"] = test_server()
        print(
            f"{total=} {solution_str=}, \n{data_source=}, \n{filtered_metrics=},\n{valid_code=} \n{extra_info['prompt']=}\nextra_info['relevance']={(extra_info['relevance'] if 'relevance' in extra_info else '')} {ground_truth=}"
        )

    return {"score": total, **metrics_to_write}


# %%
